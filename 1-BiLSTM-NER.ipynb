{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SZ5VlnTaFmo"
   },
   "source": [
    "# 1 - BiLSTM pour Named Entity Recognition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans ce notebook, nous allons implémenter un modèle LSTM multi-couche et bidirectionnel pour faire de la reconnaissance d'entités nommés (ou NER) en utilisant le dataset CONLL2003.\n",
    "\n",
    "## Préparer les données\n",
    "\n",
    "On importe tout d'abord les librairies utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7PIIl2KcaFmq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont téléchargeable [ici](https://github.com/davidsbatista/NER-datasets/tree/master/CONLL2003) au format .txt et sont sous la forme : \n",
    "\n",
    "```\n",
    "-DOCSTART- -X- -X- O\n",
    "\n",
    "SOCCER NN B-NP O\n",
    "- : O O\n",
    "JAPAN NNP B-NP B-LOC\n",
    "GET VB B-VP O\n",
    "LUCKY NNP B-NP O\n",
    "WIN NNP I-NP O\n",
    ", , O O\n",
    "CHINA NNP B-NP B-PER\n",
    "IN IN B-PP O\n",
    "SURPRISE DT B-NP O\n",
    "DEFEAT NN I-NP O\n",
    ". . O O\n",
    "\n",
    "Nadim NNP B-NP B-PER\n",
    "Ladki NNP I-NP I-PER\n",
    "\n",
    "AL-AIN NNP B-NP B-LOC\n",
    ", , O O\n",
    "United NNP B-NP B-LOC\n",
    "Arab NNP I-NP I-LOC\n",
    "Emirates NNPS I-NP I-LOC\n",
    "1996-12-06 CD I-NP O\n",
    "```\n",
    "\n",
    "où les tags qui nous intéressent pour le named entity recognition sont ceux de la dernière colonne de chaque ligne. Par exemple, 0, B-LOC, B-PER ...\n",
    "\n",
    "Nous allons arranger les données pour avoir une phrase par ligne et les convertir au format csv, pour qu'on puisse utiliser TorchText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(filename):\n",
    "    data_test = []\n",
    "    tag_test = []\n",
    "    t = []\n",
    "    sent = []\n",
    "    label = []\n",
    "    vocab = {}\n",
    "    f1  = open(filename, \"r\") \n",
    "    for i, line in enumerate(f1):  \n",
    "        if line.split(): #on ne prend pas en compte les listes vides\n",
    "            vocab[line.split()[0]] = i\n",
    "            sent.append(line.split()[0])\n",
    "            tag = [s for s in line.split()[1:] if s not in string.punctuation]\n",
    "            tag_test.append(tag[-1])\n",
    "            t.append(tag[-1])\n",
    "\n",
    "        else:\n",
    "            if tag_test:\n",
    "                label.append(\" \".join(tag_test))\n",
    "                tag_test = []\n",
    "            if sent:\n",
    "                data_test.append(\" \".join(sent))\n",
    "                sent = []\n",
    "             \n",
    "    return data_test, label\n",
    "\n",
    "word_train, tag_train = process_doc('data_ner/train.txt')\n",
    "word_val, tag_val = process_doc('data_ner/valid.txt')\n",
    "word_test, tag_test = process_doc('data_ner/test.txt')\n",
    "\n",
    "\n",
    "train = pd.DataFrame({'text':word_train, 'tag': tag_train})\n",
    "validation = pd.DataFrame({'text':word_val, 'tag': tag_val})\n",
    "test = pd.DataFrame({'text':word_test, 'tag': tag_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos données sont maintenant sous cette forme : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On les convertis en csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv (r'data_ner/train.csv', index = False, header=True)\n",
    "test.to_csv (r'data_ner/test.csv', index = False, header=True)\n",
    "validation.to_csv (r'data_ner/valid.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a alors des données en forme pour pouvoir utiliser les méthodes de TorchText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEF9O3m1aFm8"
   },
   "outputs": [],
   "source": [
    "# pour la reproductibilité\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgGjIbQjaFnE"
   },
   "source": [
    "On utilise la méthode `Field` de TorchText pour pré-traiter nos données. On utilise seulement `lower = True` pour mettre en minuscule le texte.\n",
    "\n",
    "Pour les tags, on définit également un `Field`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOgQnsbiaFnG"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower = True) \n",
    "TAG = data.Field(unk_token = None) # les tags sont tous connus on a alors unk_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "gUMdVt03aFnW",
    "outputId": "66801e7c-b944-48ca-bae9-8a90f95047f0"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path=\"./data_ner/\",\n",
    "        train=\"train.csv\",\n",
    "        validation=\"valid.csv\",\n",
    "        test=\"test.csv\", format='csv', skip_header=True,\n",
    "        fields=((\"text\", TEXT), (\"tag\", TAG))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLfP1SMNaFng"
   },
   "source": [
    "On affiche le nombre de phrases dans chaque dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ssqiO7ViaFnh",
    "outputId": "e9e7b493-818a-4490-f9cb-79523ebf6b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 14986\n",
      "Number of validation examples: 3465\n",
      "Number of testing examples: 3683\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kB0kIl5daFno"
   },
   "source": [
    "Affichons un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "G6vqkBQxaFnq",
    "outputId": "41a9ba8d-494d-4397-eabb-bc8f7edd060f"
   },
   "outputs": [],
   "source": [
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7opZSKVaFoI"
   },
   "source": [
    "Ensuite, nous construisons le vocabulaire.\n",
    "\n",
    "On importe les embeddings pré-entrainés de [GloVe](https://nlp.stanford.edu/projects/glove/).  \n",
    "\n",
    "\n",
    "`unk_init` est utilisé pour initialiser les embeddings qui ne sont pas dans le vocabulaire des embeddings pré-entraîné, on les initialise en utilisant une distribution Gaussienne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YgKDQ4QEaFoI",
    "outputId": "0d53a99f-8db1-4ee2-8ecb-703e4b8cc772"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 min_freq = MIN_FREQ, # les mots qui apparaissent moins que MIN_FREQ fois seront ignorés du vocabulaire\n",
    "                 vectors = \"glove.6B.300d\",\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "\n",
    "TAG.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Ws9tBVJXaFoR",
    "outputId": "32ae587e-fb43-416d-ae70-108c64984a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens dans le vocabulaire de TEXT vocabulary: 10952\n",
      "Unique tokens dans le vocabulaire de TAG : 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens dans le vocabulaire de TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens dans le vocabulaire de TAG : {len(TAG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut afficher les tags les plus fréquents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1KHpSqjaFoz"
   },
   "outputs": [],
   "source": [
    "def tag_percentage(tag_counts):\n",
    "    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    \n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "        \n",
    "    return tag_counts_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpbgscFOaFo4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "O\t\t170522\t\t83.4%\n",
      "B-LOC\t\t7140\t\t 3.5%\n",
      "B-PER\t\t6600\t\t 3.2%\n",
      "B-ORG\t\t6319\t\t 3.1%\n",
      "I-PER\t\t4528\t\t 2.2%\n",
      "I-ORG\t\t3704\t\t 1.8%\n",
      "B-MISC\t\t3438\t\t 1.7%\n",
      "I-LOC\t\t1157\t\t 0.6%\n",
      "I-MISC\t\t1155\t\t 0.6%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(TAG.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oojww7BfaFpC"
   },
   "source": [
    "On remarque que les tags ne sont pas équilibrés.\n",
    "\n",
    "Enfin, la dernière étape de préparation des données et de créer des itérateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tz436GNKaFpC"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMkvInHsaFpI"
   },
   "source": [
    "## Construire le modèle\n",
    "\n",
    "On définit un modèle LSTM multi-couche et bi-directionnel. L'image ci-dessous illustre l'architecture de ce modèle de manière simplifiée.\n",
    "\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/assets/pos-bidirectional-lstm.png?raw=1)\n",
    "\n",
    "L'explication détaillée est décrite [ici](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb) ou dans [cet article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdZjPt1FaFpJ"
   },
   "outputs": [],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text)) \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "   \n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l95vHkamaFpN"
   },
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gG0gwBwPaFpO"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300 # doit être le même que la dimension du GloVe embeddings\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(TAG.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = BiLSTMNER(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtEA5KgEaFpT"
   },
   "source": [
    "On initialise les poids avec une disribution Gaussienne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnCGrUG3aFpU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMNER(\n",
       "  (embedding): Embedding(10952, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0jDNLJ0aFpY"
   },
   "source": [
    "On affiche combien ce modèle a de paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BaJF29JSaFpY",
    "outputId": "2b662b54-7593-45b0-e63a-8078092cc6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 1,728,554 paramètres à entraîner\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XlHSDuwaFpd"
   },
   "source": [
    "On initialise la couche embedding du modèle avec les valeurs des embedding pré-entraînées importées précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TXt4JhmLaFpf",
    "outputId": "4708724b-2d74-4eb6-e085-2c947d39d7af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0239, -0.8160, -1.7194,  ..., -0.7362, -1.5400, -0.4917],\n",
       "        [ 0.3937,  0.3108, -0.7976,  ..., -0.5295, -0.6438,  1.2912],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.9136,  0.4711, -2.1737,  ..., -1.4350,  0.4780, -0.9254],\n",
       "        [ 0.2933, -0.5548, -0.1047,  ...,  0.3955, -0.1746, -0.1650],\n",
       "        [ 1.4863,  2.3183,  2.2479,  ...,  0.2781,  0.3719,  0.6420]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8e-eWPnaFps"
   },
   "source": [
    "On initialise également les embedding des tokens pad à zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEljX3lMaFpt"
   },
   "outputs": [],
   "source": [
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eV7ojgvAaFpx"
   },
   "source": [
    "On définit ensuite l'optimiseur. On utilise Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49hR6CitaFpx"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAe9E1BjaFp1"
   },
   "source": [
    "La fonction de coût est la cross-entropy. Dans le vocabulaire de TAG, on a des tokens `<pad>` car dans un batch les phrases doivent avoir la même taille. En revanche, nous ne voulons par calculer la perte quand le tag vaut  `<pad>`, c'est pour cela qu'on doit les ignorer avec l'argument `ignore_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uezSIvMsaFp1"
   },
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = TAG.vocab.stoi[TAG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwkIn6LnaFp6"
   },
   "source": [
    "On place notre modèle et la fonction de coût sur GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGW86nezaFp7"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRpt7qE1aFp_"
   },
   "source": [
    "La fonction suivante calcule le f1 score en ne prenant pas en compte les tokens `<pad>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eCX3l6raFqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import f1_score\\ndef f1_loss(preds, y, tag_pad_idx):\\n    #index_o = TAG.vocab.stoi[\"O\"]\\n    positive_labels = [i for i in range(len(TAG.vocab.itos))]\\n    #flatten_preds = [pred for sent_pred in preds for pred in sent_pred]\\n    #positive_preds = [pred for pred in flatten_preds\\n    #                     if pred not in (TAG_PAD_IDX, index_o)]\\n    #flatten_y = [tag for sent_tag in y for tag in sent_tag]\\n    max_preds = preds.argmax(dim = 1, keepdim = True) \\n    non_pad_elements = (y != tag_pad_idx).nonzero()\\n    y_pred = (max_preds[non_pad_elements])\\n    y_true = (y_pred == y[non_pad_elements]).float() \\n    f1 = f1_score(y_true.cpu().numpy(), y_pred.cpu().numpy(),labels=positive_labels,average=\"micro\") \\n    \\n    \\n    return f1\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_loss(preds, y, tag_pad_idx):\n",
    "    '''\n",
    "    Retourne le score F1\n",
    "    '''  \n",
    "     \n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    y_pred = (max_preds[non_pad_elements])\n",
    "    y_true = (y_pred == y[non_pad_elements]).float() \n",
    "            \n",
    "    tp = (y_true * y_pred).sum().float()\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().float()\n",
    "    fp = ((1 - y_true) * y_pred).sum().float()\n",
    "    fn = (y_true * (1 - y_pred)).sum().float()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    if (tp + fn) == 0 or (tp + fp) == 0 or (recall + precision == 0):\n",
    "        f1 = torch.zeros(1)\n",
    "    else:\n",
    "        f1 = 2* (precision*recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "from sklearn.metrics import f1_score\n",
    "def f1_loss(preds, y, tag_pad_idx):\n",
    "    index_o = TAG.vocab.stoi[\"O\"]\n",
    "    positive_labels = [i for i in range(len(TAG.vocab.itos))\n",
    "                           if i not in (tag_pad_idx, index_o)]\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    pred = pred.data.cpu().numpy() \n",
    "    tags = y.data.cpu().numpy()\n",
    "    f1 = f1_score(\n",
    "            y_true=tags,\n",
    "            y_pred=pred,\n",
    "            labels=positive_labels,\n",
    "            average=\"micro\"\n",
    "        ) \n",
    "       \n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11kQmhTXaFqD"
   },
   "source": [
    "Vient ensuite la fonction qui gère l'entraînement de notre modèle.\n",
    "\n",
    "Nous avons d'abord mis le modèle en mode `train` pour activer le dropout / batch-normarlization (si utilisé). Ensuite, nous itérons sur notre itérateur, qui renvoie un batch d'exemples.\n",
    "\n",
    "Pour chaque batch:\n",
    "- on remet à zéro les gradients sur les paramètres du dernier calcul de gradient\n",
    "- insérez le batch de texte dans le modèle pour obtenir des prédictions\n",
    "- comme les fonctions de perte de PyTorch ne peuvent pas gérer les prédictions en 3 dimensions, nous redimensionnons nos prédictions\n",
    "- calculer la perte et la précision entre les tags prédits et les tags réels\n",
    "- appeler `backward` pour calculer les gradients des paramètres w.r.t. la perte\n",
    "- effectuez une «step» d'optimisation pour mettre à jour les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_tag(predictions, tags):\n",
    "    n_tags = len(TAG.vocab)\n",
    "    class_correct = list(0 for i in range(n_tags))\n",
    "    class_total = list(0 for i in range(n_tags))\n",
    "    acc = list(0 for i in range(n_tags))\n",
    "    _, pred = torch.max(predictions, 1)\n",
    "    # # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(tags.data.view_as(pred)))\n",
    "    # # calculate test accuracy for each object class\n",
    "    for i in range(BATCH_SIZE):\n",
    "        label = tags.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "    for i in range(n_tags):\n",
    "        if np.sum(class_total[i]) == 0 and np.sum(class_correct[i]) ==0:\n",
    "            res = 100\n",
    "        else:\n",
    "            res = 100 * class_correct[i] / class_total[i]\n",
    "        acc[i] = res, np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "        \n",
    "    return acc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVTIS6jfaFqD"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "      \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        text = batch.text\n",
    "        tags = batch.tag\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        predictions = model(text)\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = accuracy_per_tag(predictions, tags)\n",
    "        f1 = f1_loss(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), acc, epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCI_HQHsaFqI"
   },
   "source": [
    "La fonction `evaluer` est similaire à la fonction` train`, sauf avec les modifications apportées afin de ne pas mettre à jour les paramètres du modèle.\n",
    "\n",
    "`model.eval ()` est utilisé pour mettre le modèle en mode évaluation, donc dropout / batch-norm / etc. sont désactivés.\n",
    "\n",
    "La boucle d'itération est également enveloppée dans `torch.no_grad` pour nous assurer que nous ne calculons aucun gradient. Nous n'avons pas non plus besoin d'appeler `optimizer.zero_grad ()` et `optimizer.step ()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-ZN-CKraFqI"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.tag\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = accuracy_per_tag(predictions,tags)\n",
    "            f1 = f1_loss(predictions, tags, tag_pad_idx)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), acc, epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJWClTfkaFqM"
   },
   "source": [
    "On affiche la perte et l'accuracy à chaque époque, ainsi que le temps d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "colab_type": "code",
    "id": "NRzu_9Y5aFqV",
    "outputId": "a16a0610-53c2-4962-a739-962da7e3aeb9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.017 | Train F1 score: 63.20%\n",
      "\t Val. Loss: 0.167 |  Val. F1 score: 64.22%\n",
      "Epoch: 02 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.016 | Train F1 score: 63.12%\n",
      "\t Val. Loss: 0.178 |  Val. F1 score: 64.21%\n",
      "Epoch: 03 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.014 | Train F1 score: 63.12%\n",
      "\t Val. Loss: 0.173 |  Val. F1 score: 62.81%\n",
      "Epoch: 04 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.013 | Train F1 score: 63.10%\n",
      "\t Val. Loss: 0.191 |  Val. F1 score: 64.22%\n",
      "Epoch: 05 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.012 | Train F1 score: 63.10%\n",
      "\t Val. Loss: 0.208 |  Val. F1 score: 64.43%\n",
      "Epoch: 06 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.011 | Train F1 score: 63.07%\n",
      "\t Val. Loss: 0.187 |  Val. F1 score: 63.56%\n",
      "Epoch: 07 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.010 | Train F1 score: 63.22%\n",
      "\t Val. Loss: 0.194 |  Val. F1 score: 64.30%\n",
      "Epoch: 08 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.010 | Train F1 score: 63.10%\n",
      "\t Val. Loss: 0.200 |  Val. F1 score: 64.28%\n",
      "Epoch: 09 | Epoch Time: 0m 3s\n",
      "\tTrain Loss: 0.009 | Train F1 score: 63.06%\n",
      "\t Val. Loss: 0.203 |  Val. F1 score: 64.07%\n",
      "Epoch: 10 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.009 | Train F1 score: 63.17%\n",
      "\t Val. Loss: 0.203 |  Val. F1 score: 63.93%\n"
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 30\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "\n",
    "    if epoch%5 == 0: \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train F1 score: {train_f1*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1 score: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy of <pad>: 100% ( 0/ 0)\n",
      "Train Accuracy of     O: 98% (77/78)\n",
      "Train Accuracy of B-LOC: 100% (13/13)\n",
      "Train Accuracy of B-PER: 100% (12/12)\n",
      "Train Accuracy of B-ORG: 100% (20/20)\n",
      "Train Accuracy of I-PER: 100% ( 0/ 0)\n",
      "Train Accuracy of I-ORG: 100% ( 0/ 0)\n",
      "Train Accuracy of B-MISC: 100% ( 5/ 5)\n",
      "Train Accuracy of I-LOC: 100% ( 0/ 0)\n",
      "Train Accuracy of I-MISC: 100% ( 0/ 0)\n"
     ]
    }
   ],
   "source": [
    "n_tags = len(TAG.vocab)\n",
    "for i in range(n_tags):   \n",
    "    print('Train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], train_acc[i][0],\n",
    "           train_acc[i][1], train_acc[i][2]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy of <pad>:  0% ( 0/19)\n",
      "Valid Accuracy of     O: 100% (93/93)\n",
      "Valid Accuracy of B-LOC: 100% ( 1/ 1)\n",
      "Valid Accuracy of B-PER: 100% ( 2/ 2)\n",
      "Valid Accuracy of B-ORG: 28% ( 2/ 7)\n",
      "Valid Accuracy of I-PER: 100% ( 2/ 2)\n",
      "Valid Accuracy of I-ORG: 50% ( 2/ 4)\n",
      "Valid Accuracy of B-MISC: 100% ( 0/ 0)\n",
      "Valid Accuracy of I-LOC: 100% ( 0/ 0)\n",
      "Valid Accuracy of I-MISC: 100% ( 0/ 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_tags):\n",
    "    print('Valid Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], valid_acc[i][0],\n",
    "           valid_acc[i][1], valid_acc[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUsyVpt3aFqZ"
   },
   "source": [
    "Nous chargeons nos meilleurs paramètres et évaluons sur les données test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MwBaIDV3aFqZ",
    "outputId": "1d2d81c8-ab83-46a0-8142-207c8a35e94f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy of <pad>:  0% ( 0/ 2)\n",
      "Train Accuracy of     O: 100% (56/56)\n",
      "Train Accuracy of B-LOC: 72% ( 8/11)\n",
      "Train Accuracy of B-PER: 66% ( 4/ 6)\n",
      "Train Accuracy of B-ORG: 93% (41/44)\n",
      "Train Accuracy of I-PER: 100% ( 0/ 0)\n",
      "Train Accuracy of I-ORG: 75% ( 3/ 4)\n",
      "Train Accuracy of B-MISC: 100% ( 2/ 2)\n",
      "Train Accuracy of I-LOC: 66% ( 2/ 3)\n",
      "Train Accuracy of I-MISC: 100% ( 0/ 0)\n",
      "Test Loss: 0.260 |  Test F1 score: 62.51%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_f1 = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "n_tags = len(TAG.vocab)\n",
    "for i in range(n_tags):   \n",
    "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], test_acc[i][0],\n",
    "           test_acc[i][1], test_acc[i][2]))\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test F1 score: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05snyKgFaFqc"
   },
   "source": [
    "## Inférence\n",
    "\n",
    "Nous définissons une fonction `tag_sentence` qui va:\n",
    "- mettre le modèle en mode évaluation\n",
    "- tokenize la phrase avec spaCy si ce n'est pas une liste\n",
    "- mettre en minuscule des tokens si le `Field` l'a fait\n",
    "- numériser les tokens en utilisant le vocabulaire\n",
    "- découvrir quels tokens ne sont pas dans le vocabulaire, c'est-à-dire sont des tokens `<unk>`\n",
    "- convertir les tokens numérisés en un tenseur et ajouter une dimension de batch\n",
    "- introduire le tenseur dans le modèle\n",
    "- obtenir les prédictions sur la phrase\n",
    "- convertir les prédictions en tags lisibles\n",
    "\n",
    "En plus de renvoyer les tokens et les tags, il renvoie également les tokens qui étaient des jetons `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYf_SRwFaFqd"
   },
   "outputs": [],
   "source": [
    "def tag_sentence(model, device, sentence, text_field, tag_field):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token for token in sentence]\n",
    "\n",
    "    if text_field.lower:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        \n",
    "    numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
    "\n",
    "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
    "    \n",
    "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "    \n",
    "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "    \n",
    "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
    "         \n",
    "    predictions = model(token_tensor)\n",
    "    \n",
    "    top_predictions = predictions.argmax(-1)\n",
    "    \n",
    "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
    "    \n",
    "    return tokens, predicted_tags, unks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oReGRyxiaFqg"
   },
   "source": [
    "On utilise un exemple du dataset train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PCwGFz4kaFqh",
    "outputId": "a5af1dc6-44b3-43a7-b1fd-d3d56dfea5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n"
     ]
    }
   ],
   "source": [
    "example_index = 1\n",
    "\n",
    "sentence = vars(train_data.examples[example_index])['text']\n",
    "actual_tags = vars(train_data.examples[example_index])['tag']\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKDzhOh9aFqk"
   },
   "source": [
    "On utilise la fonction `tag_sentence` pour trouver les tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FjPwbxnaaFqk",
    "outputId": "caff0d00-4ef9-42ed-9d1b-85e1f1f021f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rejects']\n"
     ]
    }
   ],
   "source": [
    "tokens, pred_tags, unks = tag_sentence(model, \n",
    "                                       device, \n",
    "                                       sentence, \n",
    "                                       TEXT, \n",
    "                                       TAG)\n",
    "\n",
    "print(unks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie alors si le modèle a correctement prédit les tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "ezJE_m3RaFqp",
    "outputId": "aa5b06f3-b1c6-43de-99cb-7cd55b8537c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
      "\n",
      "B-ORG\t\tB-ORG\t\t✔\t\teu\n",
      "O\t\tO\t\t✔\t\trejects\n",
      "B-MISC\t\tB-MISC\t\t✔\t\tgerman\n",
      "O\t\tO\t\t✔\t\tcall\n",
      "O\t\tO\t\t✔\t\tto\n",
      "O\t\tO\t\t✔\t\tboycott\n",
      "B-MISC\t\tB-MISC\t\t✔\t\tbritish\n",
      "O\t\tO\t\t✔\t\tlamb\n",
      "O\t\tO\t\t✔\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
    "\n",
    "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
    "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
    "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMCLE1txaFqs"
   },
   "source": [
    "Essayons avec notre propre phrase. La liste unks est nulle cela signifie que tous les mots de cette phrase sont dans le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "07FscjbZaFqt",
    "outputId": "e1f7a079-f519-4101-fd7a-3f7ff77c7bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kate']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The will deliver a speech about the conflict in North Korea tomorrow in New York with my friend Mary Kate.'\n",
    "\n",
    "tokens, tags, unks = tag_sentence(model, \n",
    "                                  device, \n",
    "                                  sentence, \n",
    "                                  TEXT, \n",
    "                                  TAG)\n",
    "\n",
    "print(unks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Z_oRSprfaFqw",
    "outputId": "2335413b-4c1c-4703-f23f-b3a7d7d88216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred. Tag\tToken\n",
      "\n",
      "O\t\tthe\n",
      "O\t\twill\n",
      "O\t\tdeliver\n",
      "O\t\ta\n",
      "O\t\tspeech\n",
      "O\t\tabout\n",
      "O\t\tthe\n",
      "O\t\tconflict\n",
      "O\t\tin\n",
      "B-LOC\t\tnorth\n",
      "I-LOC\t\tkorea\n",
      "O\t\ttomorrow\n",
      "O\t\tin\n",
      "B-LOC\t\tnew\n",
      "I-LOC\t\tyork\n",
      "O\t\twith\n",
      "O\t\tmy\n",
      "O\t\tfriend\n",
      "B-PER\t\tmary\n",
      "I-PER\t\tkate\n",
      "O\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred. Tag\\tToken\\n\")\n",
    "\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{tag}\\t\\t{token}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de 1 - BiLSTM for PoS Tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
