{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des annotations de Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tarfile\n",
    "#tf = tarfile.open(\"libex.tar.gz\")\n",
    "#tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "\n",
    "`Watson` permet de retrouver le corpus de textes utilisés pour un entrainement (le dernier a priori)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'd67b6df0-ea74-11e9-8ff4-f146741a0385'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watsonsets(corpus):\n",
    "    ws_json = os.path.join('../libex/data', 'corpus-%s' % corpus, 'sets.json')\n",
    "    print(\"watsonset file is '%s'\" % ws_json)\n",
    "    with open(ws_json) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watsonset(ws, wskey):\n",
    "    return [d for s in ws if s['name']==wskey for d in s['documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watsondocs(corpus, ws, wskey):\n",
    "    wdocs = []\n",
    "    for d in watsonset(ws, wskey):\n",
    "        json_file = os.path.join('../libex/data', 'corpus-%s' % corpus, 'gt', '%s.json' % d)\n",
    "        with open(json_file) as f:\n",
    "            wdocs.append(json.load(f))\n",
    "    print('%s documents for %s' % (len(wdocs), wskey))\n",
    "    return wdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_from_watson(corpus):\n",
    "    ws = watsonsets(corpus)\n",
    "    # train set\n",
    "    train_wd = watsondocs(corpus, ws, 'Training')\n",
    "    # test set\n",
    "    test_wd = watsondocs(corpus, ws, 'Test')\n",
    "    return train_wd, test_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watsonset file is '../libex/data/corpus-d67b6df0-ea74-11e9-8ff4-f146741a0385/sets.json'\n",
      "37 documents for Training\n",
      "12 documents for Test\n"
     ]
    }
   ],
   "source": [
    "train_wd, test_wd = train_test_from_watson(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche les labels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Locution', 'PV_unit', 'PV_val_max', 'PV_val_min', 'SA_unit', 'SA_val_max', 'SA_val_min', 'Pore_volume', 'Surface_area', 'Support', 'Catalyst']\n"
     ]
    }
   ],
   "source": [
    "ontology_json = os.path.join('../libex/data', 'types_PV_SA.json')\n",
    "with open(ontology_json) as f:\n",
    "    ontology = json.load(f)\n",
    "    \n",
    "entities = [e['label'] for e in ontology['entityTypes']]\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watson to SpaCy\n",
    "\n",
    "On convertit du format `Watson` au format `SpaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watson2spacy(watson_doc, verbose=False):\n",
    "    # split, remove last empty sentence\n",
    "    # nb: could use nltk/spacy but need to match Watson\n",
    "    sentences = watson_doc['text'].split('\\n')[:-1]\n",
    "    # cumulative lengths of sentence\n",
    "    slen = [len(s)+1 for s in sentences]\n",
    "    cum_slen = np.append([0], np.cumsum(slen))\n",
    "    # spacy doc from sentences\n",
    "    spacy_doc = [(s, {'entities': []}) for s in sentences]\n",
    "    # loop over annotated entities from watson\n",
    "    for m in watson_doc['mentions']:\n",
    "        # position indexes in the whole text\n",
    "        begin = m['begin']\n",
    "        end = m['end']\n",
    "        # find the sentence\n",
    "        sid = np.argmin(cum_slen < m['end'])-1\n",
    "        # offset to position in sentence\n",
    "        offset = cum_slen[sid]\n",
    "        # translate from text to sentence\n",
    "        begin -= offset\n",
    "        end -= offset\n",
    "        # fill spacy doc\n",
    "        spacy_doc[sid][1]['entities'].append((begin,end,m['type']))\n",
    "               \n",
    "    # spacy cant deal with overlapping entities\n",
    "    \n",
    "    for s,d in spacy_doc:\n",
    "        entities = d['entities']\n",
    "        types = set([t for _,_,t in entities])\n",
    "        # fix overlapping of same type by fusion\n",
    "        for t in types:\n",
    "            chunks = [(b,e, mt) for b,e,mt in entities if mt == t]\n",
    "            chunks = sorted(chunks, key=itemgetter(0))\n",
    "            for i, c in enumerate(chunks[:-1]):\n",
    "                nc = chunks[i+1]\n",
    "                if c[1] > nc[0]:\n",
    "                    if verbose:\n",
    "                        print(\"Overlapping in doc '%s' :\" % watson_doc['name'])\n",
    "                        print(\"'%s' at %s and '%s' at %s for type '%s'\" % (s[c[0]:c[1]], c, s[nc[0]:nc[1]], nc, t))\n",
    "                        print(\"sentence is '%s'\" % s)\n",
    "                    fix_c = (min(c[0], nc[0]), max(c[1], nc[1]), nc[2])\n",
    "                    entities.remove(c)\n",
    "                    entities[entities.index(nc)] = fix_c\n",
    "                    chunks[i+1] = fix_c # to continue loop with up to date version \n",
    "    \n",
    "    return spacy_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sd = [watson2spacy(d) for d in train_wd]\n",
    "train_sd = [d for l in train_sd for d in l]\n",
    "\n",
    "test_sd = [watson2spacy(d) for d in test_wd]\n",
    "test_sd = [d for l in test_sd for d in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I claim as my invention:', {'entities': []}),\n",
       " ('1. A method of impregnating a support which comprises contaeting a hydrogel of a refractory oxide with an aqueous solution of a heat decomposable tungsten compound in the presence of a ferrous salt.',\n",
       "  {'entities': [(30, 37, 'Support')]}),\n",
       " ('2. The method of claim 1 wherein the solution consists essentially of both ferrous salt and the tungsten compound.',\n",
       "  {'entities': []}),\n",
       " ('3. The method of claim 1 wherein fluoride is incorporated into the hydrogel by adding a water - soluble inorganic fluoride salt or hydrofluoric acid.',\n",
       "  {'entities': []})]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sd[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un dataset de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = int(len(test_sd)/2) #97\n",
    "valid_sd = test_sd[:n_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entities = [0]*len(entities)\n",
    "for s, d in test_sd: \n",
    "    if d['entities']:\n",
    "        for i in d['entities']:\n",
    "            for ent in entities:\n",
    "                n_entities[entities.index(ent)] += i[2].count(ent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Locution',\n",
       " 'PV_unit',\n",
       " 'PV_val_max',\n",
       " 'PV_val_min',\n",
       " 'SA_unit',\n",
       " 'SA_val_max',\n",
       " 'SA_val_min',\n",
       " 'Pore_volume',\n",
       " 'Surface_area',\n",
       " 'Support',\n",
       " 'Catalyst']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'entities' : entities, 'nb' : n_entities}\n",
    "df_nb_ent = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entities</th>\n",
       "      <th>nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Locution</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PV_unit</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PV_val_max</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PV_val_min</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA_unit</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SA_val_max</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SA_val_min</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pore_volume</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Surface_area</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Support</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Catalyst</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entities   nb\n",
       "0       Locution   69\n",
       "1        PV_unit   11\n",
       "2     PV_val_max    6\n",
       "3     PV_val_min    9\n",
       "4        SA_unit   17\n",
       "5     SA_val_max    5\n",
       "6     SA_val_min   14\n",
       "7    Pore_volume   14\n",
       "8   Surface_area   18\n",
       "9        Support   86\n",
       "10      Catalyst  181"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nb_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy to conll\n",
    "\n",
    "On convertit du format `SpaCy` au format `conll` où chaque ligne correspond à un mot et son entité séparés par une tabulation, et les phrases sont séparées par un saut de ligne.\n",
    "    \n",
    "```\n",
    "I    O\n",
    "claim    O\n",
    "as    O\n",
    "my    O\n",
    "invention:    O\n",
    "\n",
    "1.    O\n",
    "A    O\n",
    "method    O\n",
    "of    O\n",
    "impregnating    O\n",
    "a    O\n",
    "support    B-Support\n",
    "which    O\n",
    "\n",
    "```\n",
    "\n",
    "La fonction ci-dessous sera utilisée pour vérifiée que le nombre d'entités dans le dataset spacy est le même que dans le dataset conll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entites_spacy(d, s):\n",
    "    c = 0\n",
    "    wentities =[]\n",
    "    ent = []\n",
    "    if d['entities'] != []:\n",
    "        for l in range(len(d['entities'])):\n",
    "            b = d['entities'] [l][0]\n",
    "            end = d['entities'][l][1]\n",
    "            ent.append(d['entities'][l][2])\n",
    "            w = s[b:end]\n",
    "            wentities.append(w)\n",
    "            c = c + 1\n",
    "    return c, wentities, ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re \n",
    "def spacy2conll(spacy_doc):    \n",
    "    # liste des entités\n",
    "    ent = [] \n",
    "    # liste des revendications\n",
    "    sent = [] \n",
    "    phr = []\n",
    "    # on compte les phrases qui n'ont pas le bon nombre d'entités\n",
    "    count = 0\n",
    "    for s, d in spacy_doc:    \n",
    "        # supprimer espaces multiples          \n",
    "        s = \" \".join(s.split())\n",
    "        # transformer la chaîne de caractère en liste\n",
    "        tab_s = s.split(\" \")\n",
    "        ls = len(tab_s) \n",
    "        # on initialise une liste d'entité pour chaque phrase à 'O'\n",
    "        e = ['O'] * (ls)    \n",
    "        # ajout d'une ligne vide à la fin de chaque phrase\n",
    "\n",
    "        count = count + 1 \n",
    "        phrases = [count] * (ls)\n",
    "\n",
    "        entities = d['entities'] \n",
    "        EE = []\n",
    "        if entities:  \n",
    "            # liste des mots correspondant à une entité \n",
    "            words = []\n",
    "            # Exception pour mot contenant \"of>\"\n",
    "            OF = False\n",
    "            PHRASE = False\n",
    "            for l in range(len(entities)):\n",
    "                # début de l'entité\n",
    "                b = entities[l][0]\n",
    "                # fin de l'entité\n",
    "                end = entities[l][1] \n",
    "                if end > len(s):\n",
    "                    end = len(s)\n",
    "                # si l'entité se termine par un espace ou une ponctuation\n",
    "                punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''   \n",
    "                if s[end-1] == ' ' or s[end-1] in punctuations:\n",
    "                    b = b - 1\n",
    "                    w = s[b:end-1]\n",
    "                # si l'entité débute par un espace ou une ponctuation\n",
    "                if s[b] == ' ' or s[b] in punctuations:\n",
    "                    b = b + 1\n",
    "                    w = s[b:end-1]\n",
    "                # si l'entité se termine par un caractère seul\n",
    "                char = re.compile(\" [a-zA-Z-/*.;]$\")  \n",
    "                if char.findall(s[b:end]) != []:                    \n",
    "                    b = b - 2\n",
    "                    end = end-2\n",
    "                    w = s[b:end] \n",
    "                char = re.compile(\" [a-zA-Z-/*.;]$\")  \n",
    "                if char.findall(s[b:end]) != []:                    \n",
    "                    b = b - 2\n",
    "                    end = end-2\n",
    "                    w = s[b:end] \n",
    "                w = re.sub(r\"^\\s+|\\s+$\", \"\", s[b:end]) \n",
    "                # exception of> : \n",
    "                if \"f>\" in s[b:end] :\n",
    "                    OF = True        \n",
    "                if PHRASE == True:\n",
    "                    b = b + n + 1  \n",
    "                if w == \"ilica,\":\n",
    "                    b = b-1\n",
    "                    end = end-2\n",
    "                words.append(w)\n",
    "                \n",
    "                # si l'entité est composé d'un seul mot\n",
    "                w_tab = [i for i in w.split(' ') if i != '' and i not in punctuations]  \n",
    "                if len(w_tab) == 1:\n",
    "                    for i, j in enumerate(tab_s):\n",
    "                        if OF:\n",
    "                            if \"of>\" in tab_s[i]:\n",
    "                                del e[i]\n",
    "                                e.insert(i,\"B-\" + entities[l][2]) \n",
    "                                tab = [i.replace(\">\", \" \") for i in tab_s if \"of>\" in i]                           \n",
    "                                tab_s.insert(i+1, tab[0].split(' ')[1]) \n",
    "                                del e[i+1]\n",
    "                                e.insert(i+1,\"B-\" + entities[l+1][2])    \n",
    "                                e.insert(ls, 'O') \n",
    "                            \n",
    "                                PHRASE = True\n",
    "                                n = len(tab[0].split(' ')[1])\n",
    "                                OF = False\n",
    "                        if w.replace(\" \", \"\") in j and b == len(' '.join(tab_s[0:i])) + 1: # si plusieurs fois le même mot\n",
    "                            del e[i]\n",
    "                            e.insert(i,\"B-\" + entities[l][2])\n",
    "                          \n",
    "                            if w == 'silica':\n",
    "                                EE.append((entities[l][2], w))\n",
    "                            EE.append((entities[l][2], w))             \n",
    "                else:\n",
    "                    # Si l'entité est composé de plusieurs mots\n",
    "                    ind = -1\n",
    "                    t = w.split(' ')\n",
    "                    t = [i for i in t if i != ' ']\n",
    "                    # premier mot\n",
    "                    for i, j in enumerate(tab_s): \n",
    "                        if t[0] in j and b == len(' '.join(tab_s[0:i])) + 1:\n",
    "                            del e[i]\n",
    "                            e.insert(i, \"B-\" + entities[l][2])\n",
    "                            ind = i\n",
    "                            EE.append((entities[l][2], w))\n",
    "                            for wo in t[1:]:\n",
    "                                for m, k in enumerate(tab_s):                         \n",
    "                                    if wo in k and m == ind + 1:\n",
    "                                        del e[m]\n",
    "                                        e.insert(m, \"I-\" + entities[l][2]) \n",
    "                                        ind = m  \n",
    "            # Vérification même nombre d'entités\n",
    "    \n",
    "        # On vérifie que chaque mot a une entité\n",
    "        assert len(e) == len(tab_s) \n",
    "        phr.append(phrases)\n",
    "        ent.append(e)\n",
    "        sent.append(tab_s)\n",
    "        \n",
    "    p = [item for sublist in phr for item in sublist]\n",
    "    tokens = [item for sublist in sent for item in sublist]\n",
    "    tag = [item for sublist in ent for item in sublist]\n",
    "\n",
    "    if len(p) != len(tag):\n",
    "        n = len(tag) - len(p)\n",
    "        for i in range(n):\n",
    "            p.append(count)\n",
    "      \n",
    "    data = pd.DataFrame({'Sentence #': p, 'text':tokens, 'tag': tag})\n",
    "    return data                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = spacy2conll(train_sd)\n",
    "data_test = spacy2conll(test_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>claim</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>my</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>invention:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37160</th>\n",
       "      <td>684</td>\n",
       "      <td>methyl</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37161</th>\n",
       "      <td>684</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37162</th>\n",
       "      <td>684</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37163</th>\n",
       "      <td>684</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37164</th>\n",
       "      <td>684</td>\n",
       "      <td>pyrrolidone.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37165 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence #          text tag\n",
       "0               1             I   O\n",
       "1               1         claim   O\n",
       "2               1            as   O\n",
       "3               1            my   O\n",
       "4               1    invention:   O\n",
       "...           ...           ...  ..\n",
       "37160         684        methyl   O\n",
       "37161         684             -   O\n",
       "37162         684             2   O\n",
       "37163         684             -   O\n",
       "37164         684  pyrrolidone.   O\n",
       "\n",
       "[37165 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le tout au format tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv (r'data_brevets/data_train.csv', index = False, header=True)\n",
    "data_test.to_csv (r'data_brevets/data_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46700"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée le même format de dataset pour le jeu de données CoNLL-2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "def conlltobert(filename):\n",
    "    ent = []\n",
    "    words = []\n",
    "    nb = []\n",
    "    phr = []\n",
    "    w = []\n",
    "    s = 1\n",
    "    with open('data_ner/'+filename, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            if row:\n",
    "                phr.append(row[0])\n",
    "                w.append(row[0])\n",
    "                ent.append(row[-1])\n",
    "            if row == []:\n",
    "                words.append(phr)\n",
    "                n = len(words[s-1])\n",
    "                nb.append([s]*n)\n",
    "                s = s + 1\n",
    "                phr = []\n",
    "\n",
    "        nb.append([s]*len(phr))\n",
    "\n",
    "    #w = [item for sublist in words for item in sublist]            \n",
    "    p = [item for sublist in nb for item in sublist]    \n",
    "    data = pd.DataFrame({'Sentence #': p, 'text':w, 'tag': ent})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_c = conlltobert('train.txt')\n",
    "data_test_c = conlltobert('test.txt')\n",
    "data_valid_c = conlltobert('valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([data_valid_c,data_test_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_c.to_csv (r'data_ner/data_train.csv', index = False, header=True)\n",
    "test.to_csv (r'data_ner/data_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
