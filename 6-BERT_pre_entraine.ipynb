{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrlTQ4RsXU-V"
   },
   "source": [
    "# Fine-tuning Transformer pré-entraîné pour la reconnaissance d'entités nommées\n",
    "\n",
    "\n",
    "Dans ce notebook, nous utiliserons un modèle [Transformer](https://arxiv.org/abs/1706.03762) , plus particulièrement le modèle [BERT](https://arxiv.org/abs/1810.04805) pré-entraîné. Notre modèle sera composé du transformer et d'une simple couche linéaire.\n",
    "\n",
    "## Préparer les données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcLVRjnjXzg7"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wb-cNA1mXU-W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.optim import Adam\n",
    "from torchtext.data import Field, NestedField, BucketIterator\n",
    "from torchtext.datasets import SequenceTaggingDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gv-PYbIUXU-b"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jj4DBdJ4XU-e"
   },
   "source": [
    "Ensuite, on importe le tokenizer BERT. Cela définit la manière dont le texte dans le modèle doit être traité, mais contient surtout le vocabulaire avec lequel le modèle BERT a été pré-entraîné. Nous utiliserons le tokenizer et le modèle `bert-base-uncased`. Il a été formé sur du texte en minuscules.\n",
    "\n",
    "Afin d'utiliser des modèles pré-entraînés pour le NLP, le vocabulaire utilisé doit correspondre exactement à celui du modèle pré-entraîné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS2XMkfxXU-f"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9lRko1PXU-i"
   },
   "source": [
    "Une autre chose que nous devons faire est de nous assurer que la séquence d'entrée est formatée de la même manière que le modèle BERT a été formé.\n",
    "\n",
    "BERT a été entraîné sur des séquences commençant par un jeton `[CLS]`.\n",
    "\n",
    "Donc, la séquence des tokens :\n",
    "\n",
    "```python\n",
    "text = ['jack', 'went', 'to', 'the', 'shop']\n",
    "```\n",
    "\n",
    "doit devenir :\n",
    "\n",
    "```python\n",
    "text = ['[CLS]', 'jack', 'went', 'to', 'the', 'shop']\n",
    "```\n",
    "\n",
    "En plus de faire correspondre nos vocabulaires, nous devons également nous assurer que nos jetons padding et unk correspondent à ceux utilisés dans le modèle pré-entraîné. Par défaut, TorchText utilise `<pad>` et `<unk>`, mais le modèle BERT utilise `[PAD]` et `[UNK]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bJ7kwurvXU-j",
    "outputId": "0c0199b9-25e9-4744-8848-f3e0edf9aec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5ftgTExXU-n"
   },
   "source": [
    "Nous nous intéressons principalement aux représentations numériques des tokens spéciaux. En effet, nous n'utilisons pas le module de vocabulaire de TorchText, mais celui fourni par le modèle pré-entraîné.\n",
    "\n",
    "Nous obtenons les index des tokens spéciaux en les passant par la fonction `convert_tokens_to_ids` du tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B4TdEVVWXU-n",
    "outputId": "89801a5a-2c9f-4c7c-9508-5b8f69e05a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0XDkA4aXU-r"
   },
   "source": [
    "Une autre chose est que le modèle pré-entraîné a été formé sur des séquences d'une longueur maximale et nous devons nous assurer que nos séquences sont également coupées à cette longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zrAoyoYCXU-r",
    "outputId": "e7a0599a-3288-4f1c-e8d3-76de97e2d6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KD_ChHl0XU-v"
   },
   "source": [
    "Ensuite, nous définissons deux fonctions qui utilisent notre vocabulaire.\n",
    "\n",
    "Le premier coupera la séquence de tokens à la longueur maximale souhaitée, spécifiée par notre modèle pré-entraîné, puis convertira les tokens en index en les passant à travers le vocabulaire. C'est ce que nous allons utiliser sur notre séquence d'entrée que nous voulons tagger.\n",
    "\n",
    "Nous coupons en fait les tokens à `max_input_length-1`, car nous devons ajouter le token spécial` [CLS] `au début de la séquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_Xjhu3jXU-v"
   },
   "outputs": [],
   "source": [
    "def cut_and_convert_to_id(tokens, tokenizer, max_input_length):\n",
    "    tokens = tokens[:max_input_length-1]\n",
    "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIOm0RK-XU-y"
   },
   "source": [
    "La deuxième fonction coupe simplement la séquence à la longueur maximale. Ceci est utilisé pour nos tags. Nous ne transmettons pas les tags à travers le vocabulaire du modèle pré-entraîné, car le vocabulaire n'a été construit que pour les phrases anglaises, et non pour les tags. Nous allons alors construire nous-mêmes le vocabulaire des tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TumdN5LnXU-0"
   },
   "outputs": [],
   "source": [
    "def cut_to_max_length(tokens, max_input_length):\n",
    "    tokens = tokens[:max_input_length-1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK5u89DMXU-3"
   },
   "source": [
    "Nous devons passer les deux fonctions ci-dessus au `Field`, l'abstraction TorchText qui gère une grande partie du traitement des données pour nous. Nous utilisons les `functools` de Python qui nous permettent de passer des fonctions qui ont déjà certains de leurs arguments fournis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dgaT_fjXU-3"
   },
   "outputs": [],
   "source": [
    "text_preprocessor = functools.partial(cut_and_convert_to_id,\n",
    "                                      tokenizer = tokenizer,\n",
    "                                      max_input_length = max_input_length)\n",
    "\n",
    "tag_preprocessor = functools.partial(cut_to_max_length,\n",
    "                                     max_input_length = max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpnVGVgMXU-6"
   },
   "source": [
    "On définit les `Field` et on importe les données en utilisant ces `Field`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(use_vocab = False,\n",
    "                  lower = True,\n",
    "                  preprocessing = text_preprocessor,\n",
    "                  init_token = init_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "TAG = data.Field(unk_token = None,\n",
    "                     init_token = '<pad>',\n",
    "                     preprocessing = tag_preprocessor)\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "        path=\"data_ner/\",\n",
    "        train=\"train.csv\",\n",
    "        validation=\"valid.csv\",\n",
    "        test=\"test.csv\", format='csv', skip_header=True,\n",
    "        fields=[('text', TEXT), ('tag', TAG)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPhIecHJXU_E"
   },
   "source": [
    "Nous pouvons vérifier un exemple en l'affichant. Comme nous avons déjà numérisé notre \"texte\" en utilisant le vocabulaire du modèle pré-entraîné, il s'agit déjà d'une suite d'entiers. Les tags doivent encore être numérisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jNLMuA22XU_E",
    "outputId": "b6643217-2768-46c0-b678-a78e87cb0cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1000, 2057, 2079, 100, 2490, 2151, 2107, 12832, 2138, 2057, 2079, 100, 2156, 2151, 5286, 2005, 2009, 1010, 1000, 1996, 3222, 100, 2708, 14056, 100, 3158, 4315, 14674, 2409, 1037, 2739, 27918, 1012], 'tag': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3HiRqq8XU_H"
   },
   "source": [
    "Notre prochaine étape consiste à construire le vocabulaire des tags afin qu'elles puissent être numérisées pendant l'entraînement. Nous faisons cela en utilisant la méthode `.build_vocab` du champ sur` train_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "klpqOEHJXU_I",
    "outputId": "20ecea6e-2c5d-40f4-ef8b-6361bbfc6fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'<pad>': 0, 'O': 1, 'B-LOC': 2, 'B-PER': 3, 'B-ORG': 4, 'I-PER': 5, 'I-ORG': 6, 'B-MISC': 7, 'I-LOC': 8, 'I-MISC': 9})\n"
     ]
    }
   ],
   "source": [
    "TAG.build_vocab(train_data)\n",
    "\n",
    "print(TAG.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFetOpw2XU_L"
   },
   "source": [
    "Ensuite, nous définissons nos itérateurs. Cela définira comment les batchs de données sont fournis lors de l'entraînement. Nous définissons une taille de batch et définissons `device`, qui placera automatiquement notre batch sur le GPU, si nous en avons un.\n",
    "\n",
    "Le modèle BERT est assez grand, donc la taille du batch ici est généralement plus petite que d'habitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BgIEyEkpXU_M"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gROHWG7tXU_P"
   },
   "source": [
    "## Construire le modèle\n",
    "\n",
    "La prochaine étape consiste à définir notre modèle. Le modèle est relativement simple, avec toutes les pièces compliquées contenues dans le module BERT dont nous n'avons pas à nous soucier. Nous pouvons considérer le BERT comme une couche d'embeddings et tout ce que nous faisons est d'ajouter une couche linéaire au-dessus de ces embeddings pour prédire le tag pour chaque token dans la séquence d'entrée. \n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/assets/pos-bert.png?raw=1)\n",
    "\n",
    "Une chose à noter est que nous ne définissons pas de `embedding_dim` pour notre modèle, c'est la taille de la sortie du modèle BERT prétrainé et nous ne pouvons pas la changer. Ainsi, nous obtenons simplement `embedding_dim` de l'attribut` hidden_size` du modèle.\n",
    "\n",
    "BERT veut également des séquences avec l'élément batch en premier, c'est pourquoi nous permutons notre séquence d'entrée avant de la passer à BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bYCKE0xXU_P"
   },
   "outputs": [],
   "source": [
    "class BERTNER(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "    \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)       \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "            \n",
    "    def forward(self, text, chars):\n",
    "  \n",
    "        #text = [sent len, batch size]\n",
    "    \n",
    "        text = text.permute(1, 0)\n",
    "        #text = [batch size, sent len]  \n",
    "        embedded = self.dropout(self.bert(text)[0])\n",
    "        \n",
    "        #embedded = [batch size, seq len, emb dim]      \n",
    "        embedded = embedded.permute(1, 0, 2)           \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        predictions = self.fc(self.dropout(embedded))\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjnJmG9xXU_S"
   },
   "source": [
    "Ensuite, nous chargeons le modèle prétrainé BERT non casé - avant nous avons uniquement chargé le tokenizer associé au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q473f0kAXU_S"
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6ObUIfAXU_V"
   },
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oS-roKk0XU_V"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(TAG.vocab)\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTNER(bert,\n",
    "                      OUTPUT_DIM, \n",
    "                      DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UHh9fi8rXU_Y",
    "outputId": "89f3f275-a7d2-4e27-fd78-d110fe228735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le modèle a 109,489,930 paramètres à entraîner.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Le modèle a {count_parameters(model):,} paramètres à entraîner.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTHMkwdlXU_b"
   },
   "source": [
    "Ensuite, nous définissons notre optimiseur. Habituellement, lors du fine tuning, nous utilisons un learning rate inférieur à la normale, c'est parce que nous ne voulons pas changer radicalement les paramètres car cela peut amener notre modèle à oublier ce qu'il a appris.\n",
    "\n",
    "On prend 5e-5 (0.00005) car c'est une valeur recommandée dans le papier de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6qIKZQ2EXU_b"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsqFKO5-XU_e"
   },
   "source": [
    "Nous définissons une fonction de perte, en veillant à ignorer les pertes chaque fois que le tag cible est un token de remplissage `[PAD]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJUz954cXU_e"
   },
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = TAG.vocab.stoi[TAG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGNagpu6XU_i"
   },
   "source": [
    "Ensuite, nous plaçons le modèle sur le GPU, si nous en avons un."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ur1XIJQXU_j"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heUYqdiHXU_l"
   },
   "source": [
    "On définit une fonction qui calcule le F1 score à chaque batch, en ignorant les tokens `[PAD]` et `O`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvxKXAO5XU_m"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def f1_loss(preds, y, tag_pad_idx):\n",
    "    index_o = TAG.vocab.stoi[\"O\"]\n",
    "    positive_labels = [i for i in range(len(TAG.vocab.itos))\n",
    "                           if i not in (tag_pad_idx, index_o)]\n",
    "    _, pred = torch.max(preds, 1)\n",
    "    pred = pred.data.cpu().numpy() \n",
    "    tags = y.data.cpu().numpy()\n",
    "    f1 = f1_score(\n",
    "            y_true=tags,\n",
    "            y_pred=pred,\n",
    "            labels=positive_labels,\n",
    "            average=\"micro\"\n",
    "        ) \n",
    "       \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons également la fonction suivante pour calculer l'accuracy par tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_tag(predictions, tags):\n",
    "    n_tags = len(TAG.vocab)\n",
    "    class_correct = list(0 for i in range(n_tags))\n",
    "    class_total = list(0 for i in range(n_tags))\n",
    "    acc = list(0 for i in range(n_tags))\n",
    "    _, pred = torch.max(predictions, 1)\n",
    "    # # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(tags.data.view_as(pred)))\n",
    "    # # calculate test accuracy for each object class\n",
    "    for i in range(BATCH_SIZE):\n",
    "        label = tags.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "    for i in range(n_tags):\n",
    "        if np.sum(class_total[i]) == 0 and np.sum(class_correct[i]) ==0:\n",
    "            res = 100\n",
    "        else:\n",
    "            res = 100 * class_correct[i] / class_total[i]\n",
    "        acc[i] = res, np.sum(class_correct[i]), np.sum(class_total[i])\n",
    "        \n",
    "    return acc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5G_0OckXU_o"
   },
   "source": [
    "Nous définissons ensuite nos fonctions `train` et` evaluate` pour entraîner et tester notre modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mk9vFsMuXU_p"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        text = batch.text\n",
    "        tags = batch.tag\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        predictions = model(text)\n",
    "        \n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = accuracy_per_tag(predictions, tags)\n",
    "        f1 = f1_loss(predictions, tags, tag_pad_idx)                \n",
    "       \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), acc, epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMz_0UzGXU_s"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.tag\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = accuracy_per_tag(predictions, tags)\n",
    "            f1 = f1_loss(predictions, tags, tag_pad_idx) \n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_f1 += f1.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), acc, epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant entraîner notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7F53iNXOXU_y",
    "outputId": "c79e1589-bafb-4644-e43f-5acf8631b394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 4m 33s\n",
      "\tTrain Loss: 0.310 | Train Acc: 90.96%\n",
      "\t Val. Loss: 0.131 |  Val. Acc: 96.42%\n",
      "Epoch: 02 | Epoch Time: 3m 58s\n",
      "\tTrain Loss: 0.102 | Train Acc: 97.03%\n",
      "\t Val. Loss: 0.093 |  Val. Acc: 97.44%\n"
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    if epoch%5 == 0:\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train F1 score: {train_f1*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1 score: {valid_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche les résultats des accuracy par tags :\n",
    " - pour le train : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tags = len(TAG.vocab)\n",
    "for i in range(n_tags):   \n",
    "    print('Train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], train_acc[i][0],\n",
    "           train_acc[i][1], train_acc[i][2]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - pour les données de validation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_tags):\n",
    "    print('Valid Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], valid_acc[i][0],\n",
    "           valid_acc[i][1], valid_acc[i][2]))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - pour le test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_f1 = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "n_tags = len(TAG.vocab)\n",
    "for i in range(n_tags):   \n",
    "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "           TAG.vocab.itos[i], test_acc[i][0],\n",
    "           test_acc[i][1], test_acc[i][2]))\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test F1 score: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJ6jvG3lXU_3"
   },
   "source": [
    "## Inférence\n",
    "\n",
    "\n",
    "Nous allons maintenant voir comment utiliser notre modèle pour tagger des phrases réelles.\n",
    "\n",
    "Si nous transmettons une chaîne de caractères, cela signifie que nous devons la diviser en tokens individuels, ce que nous faisons en utilisant la fonction `tokenize` du` tokenizer`. Ensuite, on numérise les tokens de la même manière que nous l'avons fait auparavant, en utilisant `convert_tokens_to_ids`. Ensuite, nous ajoutons le token index `[CLS]` au début de la séquence.\n",
    "\n",
    "Nous passons ensuite la séquence de texte à travers notre modèle pour obtenir une prédiction pour chaque token, puis découpons les prédictions pour le token `[CLS]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIOmo6FYXU_4"
   },
   "outputs": [],
   "source": [
    "def tag_sentence(model, device, sentence, tokenizer, text_field, tag_field):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = sentence\n",
    "    \n",
    "    numericalized_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    numericalized_tokens = [text_field.init_token] + numericalized_tokens\n",
    "        \n",
    "    unk_idx = text_field.unk_token\n",
    "    \n",
    "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "    \n",
    "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "    \n",
    "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
    "         \n",
    "    predictions = model(token_tensor)\n",
    "    \n",
    "    top_predictions = predictions.argmax(-1)\n",
    "    \n",
    "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
    "    \n",
    "    predicted_tags = predicted_tags[1:]\n",
    "        \n",
    "    assert len(tokens) == len(predicted_tags)\n",
    "    \n",
    "    return tokens, predicted_tags, unks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEbdylXQXU_6"
   },
   "source": [
    "Nous pouvons ensuite exécuter des exemples dans notre modèle et afficher les tags prédits.\n",
    "\n",
    " - Exemple issu de notre jeu de données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NAjXntVXU_7"
   },
   "outputs": [],
   "source": [
    "example_index = 1\n",
    "\n",
    "sentence = vars(train_data.examples[example_index])['text']\n",
    "actual_tags = vars(train_data.examples[example_index])['tag']\n",
    "\n",
    "print(sentence)\n",
    "tokens, pred_tags, unks = tag_sentence(model, \n",
    "                                       device, \n",
    "                                       sentence, \n",
    "                                       TEXT, \n",
    "                                       TAG,\n",
    "                                       CHAR)\n",
    "\n",
    "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
    "\n",
    "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
    "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
    "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Exemple quelconque :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The will deliver a speech about the conflict in Sao Paulo at tomorrow in Anne Mary with Jack.'\n",
    "\n",
    "tokens, tags, unks = tag_sentence(model, \n",
    "                                  device, \n",
    "                                  sentence, \n",
    "                                  TEXT, \n",
    "                                  TAG,\n",
    "                                  CHAR)\n",
    "\n",
    "print(unks)\n",
    "print(\"Pred. Tag\\tToken\\n\")\n",
    "\n",
    "\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{tag}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHfrCjgIXU_-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred. Tag\tToken\n",
      "\n",
      "O\t\tthe\n",
      "O\t\tqueen\n",
      "O\t\twill\n",
      "O\t\tdeliver\n",
      "O\t\ta\n",
      "O\t\tspeech\n",
      "O\t\tabout\n",
      "O\t\tthe\n",
      "O\t\tconflict\n",
      "O\t\tin\n",
      "B-LOC\t\tnorth\n",
      "I-LOC\t\tkorea\n",
      "O\t\tat\n",
      "O\t\t1\n",
      "O\t\t##pm\n",
      "O\t\ttomorrow\n",
      "O\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred. Tag\\tToken\\n\")\n",
    "\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{tag}\\t\\t{token}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "2 - Fine-tuning Pretrained Transformers for PoS Tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
